{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.485 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "import jieba\n",
    "\n",
    "# 加载问题数据\n",
    "df1 = pd.read_csv('QA_newsys.csv', usecols=['question'])\n",
    "\n",
    "# 文本清理函数\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # 移除标点符号\n",
    "    text = text.upper()  #统一转为大写\n",
    "    return text\n",
    "\n",
    "def chinese_tokenizer(text):\n",
    "    # 使用Jieba分词\n",
    "    return \" \".join(jieba.cut(text))\n",
    "\n",
    "df1['cleaned_question'] = df1['question'].apply(clean_text)\n",
    "df1['tokenized_question'] = df1['cleaned_question'].apply(chinese_tokenizer)\n",
    "\n",
    "# 分割训练和测试集\n",
    "#train, test = train_test_split(df1, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加载预训练bert模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/dev/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "# 加载预训练BERT模型和分词器\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# 转换为BERT输入格式\n",
    "def encode_question(question):\n",
    "    inputs = tokenizer(question, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n",
    "    return inputs\n",
    "\n",
    "# 获取句子向量\n",
    "def get_sentence_embedding(question):\n",
    "    inputs = encode_question(question)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    # BERT最后一层的CLS token输出作为句子的表示\n",
    "    sentence_embedding = outputs.last_hidden_state[:, 0, :]\n",
    "    return sentence_embedding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STS语义相似度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/dev/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STS Score: 0.6316959261894226\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# 初始化模型\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "            \n",
    "def compute_sts(sentence1, sentence2):\n",
    "    # 计算句子的嵌入\n",
    "    embedding1 = model.encode(sentence1, convert_to_tensor=False)\n",
    "    embedding2 = model.encode(sentence2, convert_to_tensor=False)\n",
    "\n",
    "    #将嵌入reshape为二维数组\n",
    "    embedding1 = embedding1.reshape(1, -1)\n",
    "    embedding2 = embedding2.reshape(1, -1)\n",
    "    \n",
    "    # 计算余弦相似度\n",
    "    similarity = cosine_similarity(embedding1, embedding2)\n",
    "    return similarity[0][0]\n",
    "\n",
    "# 示例使用\n",
    "sentence1 = \"IFS的多次曝光光谱数据是如何合成的？\"\n",
    "sentence2 = \"MCI伴随图像处理的输入是什么？\"\n",
    "sts_score = compute_sts(sentence1, sentence2)\n",
    "print(f\"STS Score: {sts_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 相似度计算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试样例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Score: [[0.95451903]]\n",
      "embedding1.shape: torch.Size([1, 768])\n",
      "embedding2.shape: torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "sentence1 = \"IFS的多次曝光光谱数据是如何合成的？\"\n",
    "sentence2 = \"MCI伴随图像处理的输入是什么？\"\n",
    "embedding1 = get_sentence_embedding(sentence1)\n",
    "embedding2 = get_sentence_embedding(sentence2)\n",
    "bert_score = cosine_similarity(embedding1, embedding2)\n",
    "\n",
    "print(f\"BERT Score: {bert_score}\")\n",
    "print(f\"embedding1.shape: {embedding1.shape}\")\n",
    "print(f\"embedding2.shape: {embedding2.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取所有问题的句子向量\n",
    "question_embeddings = []\n",
    "for question in df1['tokenized_question']:\n",
    "    embedding = get_sentence_embedding(question)\n",
    "    question_embeddings.append(embedding)\n",
    "\n",
    "# 将向量转为Tensor形式\n",
    "question_embeddings = torch.cat(question_embeddings, dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended Questions and Similarity Scores:\n",
      "Question: IFS外部定标参考文件是如何生成的？, Similarity Score: 0.9544\n",
      "Question: IFS科学数据生成的主要步骤有哪些？, Similarity Score: 0.9469\n",
      "Question: IFS内部定标和外部定标中的查考文件如何比较和校验？, Similarity Score: 0.9447\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# 获取用户输入问题的向量表示\n",
    "user_question = \"IFS外部定标参考文件是如何生成的？\"\n",
    "user_embedding = get_sentence_embedding(user_question)\n",
    "\n",
    "# 计算用户问题与所有问题的余弦相似度\n",
    "similarities = cosine_similarity(user_embedding, question_embeddings)\n",
    "\n",
    "# 找出相似度最高的几个问题\n",
    "top_k = 3\n",
    "top_k_indices = similarities.argsort()[0][-top_k:][::-1]  # 按照相似度排序并取前5个\n",
    "\n",
    "# 输出推荐问题\n",
    "print(\"Recommended Questions and Similarity Scores:\")\n",
    "for idx in top_k_indices:\n",
    "    question = df1.iloc[idx]['question']\n",
    "    score = similarities[0][idx]\n",
    "    print(f\"Question: {question}, Similarity Score: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/dev/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended Questions and Similarity Scores:\n",
      "Question: IFS外部定标参考文件是如何生成的？, Similarity Score: 0.9920\n",
      "Question: IFS内部定标和外部定标中的查考文件如何比较和校验？, Similarity Score: 0.9585\n",
      "Question: IFS（积分视场光谱仪）是什么设施的重要组成部分？, Similarity Score: 0.9364\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# 用户提出的问题\n",
    "user_question = \"IFS外部定标参考文件是如何生成的？\"\n",
    "\n",
    "# 初始化SentenceTransformer模型\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "# 对用户的问题进行嵌入\n",
    "user_embedding2 = model.encode(user_question, convert_to_tensor=False).reshape(1, -1)\n",
    "\n",
    "# 对df1['tokenized_question']中的每个问题进行嵌入\n",
    "question_embeddings2 = model.encode(df1['tokenized_question'].tolist(), convert_to_tensor=False)\n",
    "\n",
    "# 计算用户问题与每个问题的余弦相似度\n",
    "similarities2 = cosine_similarity(user_embedding2, question_embeddings2).flatten()  # 转换为1D数组\n",
    "\n",
    "# 找出相似度最高的三个问题\n",
    "top_k = 3\n",
    "top_k_indices = similarities2.argsort()[-top_k:][::-1]  # 按相似度降序排列并取前3个\n",
    "\n",
    "# 输出推荐的相似问题及其相似度\n",
    "recommended_questions = df1.iloc[top_k_indices]['question']\n",
    "recommended_similarities2 = similarities2[top_k_indices]\n",
    "\n",
    "# 打印相似度最高的三个问题及其相似度\n",
    "print(\"Recommended Questions and Similarity Scores:\")\n",
    "for question, similarity in zip(recommended_questions, recommended_similarities2):\n",
    "    print(f\"Question: {question}, Similarity Score: {similarity:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 相似度计算方法测评"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from rouge import Rouge\n",
    "from bert_score import score as bert_score\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_responses(predictions, references):\n",
    "    # 初始化评估指标\n",
    "    bleu_scores = []\n",
    "    rouge = Rouge()\n",
    "    rouge_scores = []\n",
    "    exact_matches = []\n",
    "\n",
    "    for pred, ref in zip(predictions, references):\n",
    "        # BLEU Score\n",
    "        bleu_score_value = sentence_bleu([ref.split()], pred.split())\n",
    "        bleu_scores.append(bleu_score_value)\n",
    "\n",
    "        # ROUGE Score\n",
    "        rouge_score_value = rouge.get_scores(pred, ref)[0]\n",
    "        rouge_scores.append(rouge_score_value)\n",
    "\n",
    "        # Exact Match\n",
    "        exact_match = 1 if pred == ref else 0\n",
    "        exact_matches.append(exact_match)\n",
    "\n",
    "    # BERTScore\n",
    "    P, R, F1 = bert_score(predictions, references, lang=\"zh\", verbose=True)\n",
    "    bert_scores = F1.tolist()\n",
    "\n",
    "    # 计算平均得分\n",
    "    avg_bleu = np.mean(bleu_scores)\n",
    "    avg_rouge = {key: np.mean([score[key]['f'] for score in rouge_scores]) for key in rouge_scores[0]}\n",
    "    avg_exact_match = np.mean(exact_matches)\n",
    "    avg_bert_score = np.mean(bert_scores)\n",
    "\n",
    "    results = {\n",
    "        \"BLEU\": avg_bleu,\n",
    "        \"ROUGE\": avg_rouge,\n",
    "        \"Exact Match\": avg_exact_match,\n",
    "        \"BERTScore\": avg_bert_score\n",
    "    }\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/dev/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'question': 'IFS外部定标参考文件是如何生成的？', 'similarity_score': 0.9543768}, {'question': 'IFS科学数据生成的主要步骤有哪些？', 'similarity_score': 0.94692516}, {'question': 'IFS内部定标和外部定标中的查考文件如何比较和校验？', 'similarity_score': 0.9447235}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nif __name__ == '__main__':\\n    app.run(debug=True)\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "import re\n",
    "import jieba\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "# app = Flask(__name__)\n",
    "\n",
    "# 文本清理函数\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # 移除标点符号\n",
    "    text = text.upper()  #统一转为大写\n",
    "    return text\n",
    "\n",
    "def chinese_tokenizer(text):\n",
    "    # 使用Jieba分词\n",
    "    return \" \".join(jieba.cut(text))\n",
    "\n",
    "# 转换为BERT输入格式\n",
    "def encode_question(question):\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    inputs = tokenizer(question, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n",
    "    return inputs\n",
    "\n",
    "# 获取句子向量\n",
    "def get_sentence_embedding(question):\n",
    "    inputs = encode_question(question)\n",
    "    model = BertModel.from_pretrained('bert-base-uncased')\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    # BERT最后一层的CLS token输出作为句子的表示\n",
    "    sentence_embedding = outputs.last_hidden_state[:, 0, :]\n",
    "    return sentence_embedding\n",
    "\n",
    "# 加载问题库\n",
    "df1 = pd.read_csv('QA_newsys.csv', usecols=['question'])\n",
    "df1['cleaned_question'] = df1['question'].apply(clean_text)\n",
    "df1['tokenized_question'] = df1['cleaned_question'].apply(chinese_tokenizer)\n",
    "\n",
    "# 获取所有问题的句子向量\n",
    "question_embeddings = []\n",
    "for question in df1['tokenized_question']:\n",
    "    embedding = get_sentence_embedding(question)\n",
    "    question_embeddings.append(embedding)\n",
    "\n",
    "# 将向量转为Tensor形式\n",
    "question_embeddings = torch.cat(question_embeddings, dim=0)\n",
    "\n",
    "'''\n",
    "@app.route('/recommend', methods=['POST'])\n",
    "def recommend():\n",
    "    data = request.json\n",
    "    user_question = data['question']\n",
    "'''\n",
    "# 测试用户问题\n",
    "user_question = \"IFS外部定标参考文件是如何生成的？\"\n",
    "user_embedding = get_sentence_embedding(user_question)\n",
    "\n",
    "# 计算用户问题与所有问题的余弦相似度\n",
    "similarities = cosine_similarity(user_embedding, question_embeddings)\n",
    "\n",
    "# 找出相似度最高的几个问题\n",
    "top_k = 3\n",
    "top_k_indices = similarities.argsort()[0][-top_k:][::-1]  # 按照相似度排序并取前5个\n",
    "\n",
    "# 输出推荐问题\n",
    "recommended_questions = []\n",
    "for idx in top_k_indices:\n",
    "    question = df1.iloc[idx]['question']\n",
    "    score = similarities[0][idx]\n",
    "    recommended_questions.append({'question': question, 'similarity_score': score})\n",
    "    \n",
    "print(recommended_questions)\n",
    "\n",
    "'''\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True)\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
